# AMZN Stock Forecasting with Docker, Airflow, PostgreSQL, and HGBR

## ğŸ“Œ Project Overview
This project automates stock price prediction for **Amazon (AMZN)** using **Apache Airflow**, **Docker**, **PostgreSQL**, and **HistGradientBoostingRegressor (HGBR)**. The pipeline:
- Retrieves stock data from the **Alpha Vantage API**.
- Stores and manages all data in **PostgreSQL**.
- Computes **technical indicators** (SMA, RSI, volatility, OBV).
- Trains an ML model with **HGBR and hyperparameter tuning**.
- Predicts stock prices and evaluates model performance.
- Generates a **visual comparison** of actual vs. predicted prices with key metrics (**RÂ², MSE, MAE**).

The workflow is fully **containerized using Docker**, ensuring **scalability, reproducibility, and easy deployment**.

---

## ğŸ“Œ Data Source: Alpha Vantage API
The pipeline fetches **daily stock price data** from [Alpha Vantage API](https://www.alphavantage.co/documentation/) using the `TIME_SERIES_DAILY` function.

### **Example API Request**
```bash
https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=AMZN&apikey=YOUR_API_KEY&outputsize=full&datatype=json
```
**Extracted Features:**
- **Market data:** `open`, `high`, `low`, `close`, `volume`
- **Computed indicators:** `SMA (7 & 14 days)`, `volatility`, `RSI`, `OBV`

To use the API, obtain a **free API key** from [Alpha Vantage](https://www.alphavantage.co/support/#api-key).

---

## ğŸ“Œ Project Structure
```
â”œâ”€â”€ dags/                          # Airflow DAGs
â”‚   â”œâ”€â”€ amzn_forecast_pipeline.py  # DAG definition
â”œâ”€â”€ logs/                          # Airflow logs
â”œâ”€â”€ models/                        # Trained ML models
â”‚   â”œâ”€â”€ amzn_model.pkl             # Saved model
â”œâ”€â”€ plots/                         # Visualization output
â”‚   â”œâ”€â”€ amzn_predictions.png       # Actual vs. predicted prices
â”œâ”€â”€ scripts/                       # Python scripts
â”‚   â”œâ”€â”€ extract_data.py            # Fetches AMZN stock data from API
â”‚   â”œâ”€â”€ preprocess_data.py         # Processes stock data and computes indicators
â”‚   â”œâ”€â”€ train_model.py             # Trains HGBR model and stores metrics
â”‚   â”œâ”€â”€ predict.py                 # Generates predictions using the model
â”‚   â”œâ”€â”€ visualize_predictions.py   # Plots results and saves visualization
â”‚   â”œâ”€â”€ init_db.sql                # SQL script to initialize database schema
â”œâ”€â”€ docker-compose.yaml            # Docker setup
â”œâ”€â”€ requirements.txt               # Dependencies
â”œâ”€â”€ airflow.cfg                    # Airflow configuration file
â”œâ”€â”€ README.md                      # Documentation
```

---

## ğŸ“Œ Pipeline Workflow
This **end-to-end ML pipeline** runs in **Apache Airflow** and stores all data in **PostgreSQL**:

1ï¸âƒ£ **Extract Data** â†’ Fetch AMZN stock data from API and store it in PostgreSQL (`extract_data.py`).  
2ï¸âƒ£ **Preprocess Data** â†’ Clean data, compute indicators, and update the database (`preprocess_data.py`).  
3ï¸âƒ£ **Train Model** â†’ Train HGBR with hyperparameter tuning and store metrics (`train_model.py`).  
4ï¸âƒ£ **Make Predictions** â†’ Predict stock prices and store results (`predict.py`).  
5ï¸âƒ£ **Visualize Results** â†’ Generate plots and save them in `plots/` (`visualize_predictions.py`).  

### **Airflow DAG execution order**
```bash
extract_data >> preprocess_data >> train_model >> predict >> visualize_predictions
```

---

## ğŸ“Œ PostgreSQL Database Schema
All data is **stored and managed in PostgreSQL**.

### **Database Tables**
| Table Name      | Description                                   |
|----------------|-----------------------------------------------|
| `stock_data`   | Raw stock data fetched from Alpha Vantage API |
| `processed_data` | Preprocessed data with computed indicators  |
| `predictions`  | Model predictions and actual stock prices    |
| `model_metrics` | Model performance metrics (RÂ², MSE, MAE)    |

The database is **automatically created** inside the PostgreSQL container when Airflow runs the pipeline.


---

## ğŸ“Œ Setup and Execution Guide

### 1ï¸âƒ£ Clone the Repository
To get started, clone the repository:

```bash
git clone https://github.com/FranRguezCer/amzn-stocks-predictions-airflow.git
cd amzn-stocks-predictions-airflow
```

### 2ï¸âƒ£ Set Up Docker and Airflow
Ensure **Docker** and **Docker Compose** are installed. Then, start all services, including Airflow and PostgreSQL:

```bash
docker-compose up -d # (Start all containers in detached mode)
```

### 3ï¸âƒ£ Initialize the Database
Before running the DAG, execute the SQL script to set up the database schema:

```bash
docker exec -it airflow_postgres psql -U airflow -d airflow -f /opt/airflow/scripts/init_db.sql
```

### 4ï¸âƒ£ Access Airflow Web UI
Once running, open your browser and go to:

```bash
http://localhost:8080
```

- **DAG Name:** `amzn_forecast_pipeline`
- Enable the DAG and trigger a **manual run**.

### 5ï¸âƒ£ View Results
Once the DAG completes execution:
- **Predictions Table:** Stored in PostgreSQL (`predictions` table).
- **Model Metrics Table:** Stored in PostgreSQL (`model_metrics` table).
- **Visualization Output:** Saved as an image in `plots/amzn_predictions.png`.

---

## ğŸ“Œ Model Performance Evaluation
Evaluated on **20% test data**, key metrics include:
- **RÂ² Score**: Accuracy of predictions.
- **MSE (Mean Squared Error)**: Error magnitude.
- **MAE (Mean Absolute Error)**: Average deviation.

Metrics are **displayed on the visualization**, centered at the top.

---

## ğŸ“Œ Dependency Management with Docker
You **do not need to install dependencies manually**. Docker Compose installs them inside the container and also initializes the PostgreSQL database schema automatically.

### ğŸ›  Database Initialization on Startup
The `init_db.sql` script is executed automatically when the **PostgreSQL container** starts, ensuring that all necessary tables are created before running the pipeline.

```yaml
airflow_postgres:
  image: postgres:13
  container_name: airflow_postgres
  restart: always
  environment:
    POSTGRES_USER: airflow
    POSTGRES_PASSWORD: airflow
    POSTGRES_DB: airflow
  volumes:
    - ./scripts/init_db.sql:/docker-entrypoint-initdb.d/init_db.sql
  ports:
    - "5432:5432"
```

### ğŸ›  Worker Dependency Management
Airflow Worker automatically installs all required Python dependencies before executing tasks. This ensures that all scripts have access to necessary libraries without manual intervention.

```yaml
airflow_worker:
  container_name: airflow_worker
  <<: *airflow-common
  command: celery worker
  restart: always
  depends_on:
    airflow_init:
      condition: service_completed_successfully
  entrypoint: >
    sh -c "pip install -r /opt/airflow/requirements.txt && exec airflow celery worker"
```

### ğŸ”¹ This configuration:
- **Installs all Python dependencies** listed in `requirements.txt` inside the container.
- **Ensures Airflow starts correctly** with all necessary packages.
- **Prevents compatibility issues** by keeping dependencies isolated within Docker.

### âŒ No Need for Manual Installation
Do **not** run:
```bash
pip install -r requirements.txt
```
This is already handled inside the Docker container.

Additionally, there is **no need to manually create database tables**. The initialization script (`init_db.sql`) is executed automatically when the **PostgreSQL** service starts.

---

### âš ï¸ When to Install Manually?
Only if:
- Running scripts **outside Docker**.
- Adding a new library **without restarting containers**.
- Manually modifying or resetting the **PostgreSQL** database schema.

Manual install:
```bash
pip install -r requirements.txt
```

To manually reset the database schema:
```bash
docker exec -it airflow_postgres psql -U airflow -d airflow -f /opt/airflow/scripts/init_db.sql
```

---

### ğŸš€ Conclusion
With Docker Compose, dependencies and database initialization are fully managed **inside the container**. To start the project:
```bash
docker-compose up -d
```

This will:
âœ” Install all required **Python dependencies**.  
âœ” Ensure **Airflow services** start correctly.  
âœ” Automatically **create and initialize** the **PostgreSQL** database schema.

ğŸ”¹ **Your Airflow pipeline is fully functional inside the Docker environment!** ğŸš€

---

## ğŸ“Œ Example Visualization Output
The pipeline generates a **comparison plot**:

![AMZN Stock Prediction](plots/amzn_predictions.png)

Metrics **(RÂ², MSE, MAE)** are displayed at the top.

---

## ğŸ“Œ Future Improvements
ğŸš€ **Next steps:**
- Optimize hyperparameters using **Bayesian Search**.
- Add more **market indicators** (e.g., Bollinger Bands).
- Implement **LSTM/Transformer models** for better forecasting.

---

## ğŸ“Œ License
This project is open-source under the **MIT License**.

---

**Author:** Francisco JosÃ© RodrÃ­guez Cerezo  
ğŸ“§ Contact: [portfolio.fjrguezcer@gmail.com](mailto:portfolio.fjrguezcer@gmail.com)  
ğŸ‘‰ GitHub: [FranRguezCer](https://github.com/FranRguezCer/)
